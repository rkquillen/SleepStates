{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d214f32-4c73-4410-b1e7-47f10e842f48",
   "metadata": {},
   "source": [
    "# Setup/Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456cca06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:14.781327Z",
     "iopub.status.busy": "2023-11-14T22:36:14.780811Z",
     "iopub.status.idle": "2023-11-14T22:36:15.135027Z",
     "shell.execute_reply": "2023-11-14T22:36:15.134389Z",
     "shell.execute_reply.started": "2023-11-14T22:36:14.781305Z"
    },
    "papermill": {
     "duration": 0.005585,
     "end_time": "2023-10-04T21:59:39.584958",
     "exception": false,
     "start_time": "2023-10-04T21:59:39.579373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### run this cell on Paperspace to deal with issues not present on Kaggle\n",
    "# Import score function when using paperspace\n",
    "%run /notebooks/event-detection-ap.ipynb\n",
    "competition_score = score\n",
    "# suppress TF placeholder value warning on paperspace\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3adbf7b8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:15.136862Z",
     "iopub.status.busy": "2023-11-14T22:36:15.136198Z",
     "iopub.status.idle": "2023-11-14T22:36:17.935268Z",
     "shell.execute_reply": "2023-11-14T22:36:17.934658Z",
     "shell.execute_reply.started": "2023-11-14T22:36:15.136842Z"
    },
    "papermill": {
     "duration": 8.704267,
     "end_time": "2023-10-04T21:59:39.573088",
     "exception": false,
     "start_time": "2023-10-04T21:59:30.868821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# from metric import score as competition_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10e516b-4a6b-458d-87fc-a7c1aac166b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:17.936880Z",
     "iopub.status.busy": "2023-11-14T22:36:17.936636Z",
     "iopub.status.idle": "2023-11-14T22:36:19.167349Z",
     "shell.execute_reply": "2023-11-14T22:36:19.166773Z",
     "shell.execute_reply.started": "2023-11-14T22:36:17.936857Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: W&B API key is configured. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "### get WandB api key from bash script on Paperspace\n",
    "LOG_WANDB = True\n",
    "if LOG_WANDB:\n",
    "    envs=  !bash /notebooks/sleepstates.sh\n",
    "    wandb.login(key=envs.l[0])\n",
    "    \n",
    "### prevent tf warning spam in Paperspace environment\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687e00f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:19.169017Z",
     "iopub.status.busy": "2023-11-14T22:36:19.168609Z",
     "iopub.status.idle": "2023-11-14T22:36:19.171935Z",
     "shell.execute_reply": "2023-11-14T22:36:19.171486Z",
     "shell.execute_reply.started": "2023-11-14T22:36:19.168999Z"
    },
    "papermill": {
     "duration": 0.01248,
     "end_time": "2023-10-04T21:59:39.625332",
     "exception": false,
     "start_time": "2023-10-04T21:59:39.612852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/notebooks/data\"\n",
    "TRAINING_DIR = \"/notebooks/5feat-series_data\"\n",
    "OUTPUT_DIR = \"/notebooks/output\"\n",
    "EVENTS_PATH = \"/notebooks/input/train_events.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff8560",
   "metadata": {
    "papermill": {
     "duration": 0.005557,
     "end_time": "2023-10-04T21:59:39.636517",
     "exception": false,
     "start_time": "2023-10-04T21:59:39.630960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e25fee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:19.172692Z",
     "iopub.status.busy": "2023-11-14T22:36:19.172545Z",
     "iopub.status.idle": "2023-11-14T22:36:42.596024Z",
     "shell.execute_reply": "2023-11-14T22:36:42.595344Z",
     "shell.execute_reply.started": "2023-11-14T22:36:19.172679Z"
    },
    "papermill": {
     "duration": 0.013703,
     "end_time": "2023-10-04T21:59:39.655811",
     "exception": false,
     "start_time": "2023-10-04T21:59:39.642108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mquillen\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20231114_223619-7xcbce1w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/quillen/SleepStates-SegFormer-v1/runs/7xcbce1w' target=\"_blank\">chocolate-gorge-160</a></strong> to <a href='https://wandb.ai/quillen/SleepStates-SegFormer-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/quillen/SleepStates-SegFormer-v1' target=\"_blank\">https://wandb.ai/quillen/SleepStates-SegFormer-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/quillen/SleepStates-SegFormer-v1/runs/7xcbce1w' target=\"_blank\">https://wandb.ai/quillen/SleepStates-SegFormer-v1/runs/7xcbce1w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GPU_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "LABEL_DOWNSAMPLE = 6\n",
    "\n",
    "CFG = { \"dataset\": \"full-series--clip3-5feat-12tol\",\n",
    "        \"samples_per_day\": 12, # used to create windowed dataset from each series.\n",
    "        \"n_feats\": 5,\n",
    "       ### model_hparams\n",
    "        \"patches\":[LABEL_DOWNSAMPLE+4,8,8,8],\n",
    "        \"strides\":[LABEL_DOWNSAMPLE,4,4,3], \n",
    "        \"channels\":[12,24,48,96],\n",
    "        \"enc_layers\":[2,2,2,2], \n",
    "        \"heads\":[4,4,4,4], \n",
    "        \"r_ratios\":[1,1,1,1], # if 1, efficient-MHSA will be replaced with \"vanilla\" MHSA\n",
    "        \"exp_ratios\":[6,6,6,6],\n",
    "        \"d_decoder\":64,\n",
    "\n",
    "        'block_size': 17280,\n",
    "        'label_agg_size':LABEL_DOWNSAMPLE,\n",
    "       \n",
    "        'mha_dropout': 0,\n",
    "        \"fuse_dropout\":0.1,\n",
    "        \"decoder_dropout\":0.2,\n",
    "        \"encoder_dropout\":0.2,\n",
    "\n",
    "        \"lr\":2e-3,\n",
    "        \"loss_fn\": \"CCE\",\n",
    "\n",
    "        \"lr_scheduler\": \"cos\",\n",
    "        \"epochs\":10,\n",
    "        \"kfolds\": 10,\n",
    "        \"ONE_FOLD\":True, # if true, stop training after first fold complete\n",
    "        \"class_weights\":[1.0,1.0,1.0], ## unweighted\n",
    "\n",
    "        ### Dataset hparams\n",
    "        \"roll\":1,\n",
    "        \"drop_series\":[],\n",
    "        \"restricted_series\": {\n",
    "            \"04f547b8017d\":(50_000,-1),\n",
    "            \"31011ade7c0a\":(0,190_000),\n",
    "            \"05e1944c3818\": (0,142_500),\n",
    "            \"fe90110788d2\": (12_000,-1),\n",
    "            \"fcca183903b7\": (13_000,-1),\n",
    "            \"f6d2cc003183\": (15_000,-1),\n",
    "            \"a596ad0b82aa\": (0,232_500),\n",
    "            \"854206f602d0\": (15_000,-1),\n",
    "        },\n",
    "        \"label_threshold\": 0.6,\n",
    "        \"label_smoothing\": 0,\n",
    "        # post-processing hparams\n",
    "        \"allow_unpaired_preds\": True, # e.g., Onset with no corresponding Wakeup \n",
    "        \"logit_threshold\": 0.0, # ignore logits that don't exceed this threshold\n",
    "        \"min_pred_distance\": 30,\n",
    "        \"postproc_strategy\": \"alt\",  # unused param, purely for experiment tracking\n",
    "\n",
    "        'TPU': False,\n",
    "        'GPU_BATCH_SIZE':GPU_BATCH_SIZE,\n",
    "        'model_name':\"TF SegFormer - mhsa\"\n",
    "        }\n",
    "if CFG[\"TPU\"]:\n",
    "    GPU_BATCH_SIZE *= 8\n",
    "    \n",
    "if LOG_WANDB: wandb.init(project=\"SleepStates-SegFormer-v1\",config=CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f34e35",
   "metadata": {
    "papermill": {
     "duration": 0.00549,
     "end_time": "2023-10-04T21:59:39.667427",
     "exception": false,
     "start_time": "2023-10-04T21:59:39.661937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate GroupKFold train/val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb594dc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.597699Z",
     "iopub.status.busy": "2023-11-14T22:36:42.597517Z",
     "iopub.status.idle": "2023-11-14T22:36:42.652945Z",
     "shell.execute_reply": "2023-11-14T22:36:42.652291Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.597683Z"
    },
    "papermill": {
     "duration": 0.087743,
     "end_time": "2023-10-04T21:59:39.760746",
     "exception": false,
     "start_time": "2023-10-04T21:59:39.673003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create tf dataset from np.arrays on disk.  dataset contains (X, y) tuples\n",
    "df_file_list = (pd.read_csv(f\"{DATA_DIR}/filepaths.csv\")\n",
    "                    .query(\"split > 0\")\n",
    "                    .assign(file_name = lambda x: x[\"window\"].astype(str)+\"/\"+x[\"series_id\"]+\"_\"+x[\"split\"].astype(str)+\".npy\")\n",
    ")\n",
    "for s in CFG[\"drop_series\"]:\n",
    "    df_file_list = df_file_list[df_file_list.series_id != s]\n",
    "    print(f\"dropped {s}\")\n",
    "file_paths = df_file_list.file_name\n",
    "pd_series_ids = df_file_list.series_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7831586b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.654164Z",
     "iopub.status.busy": "2023-11-14T22:36:42.653948Z",
     "iopub.status.idle": "2023-11-14T22:36:42.674303Z",
     "shell.execute_reply": "2023-11-14T22:36:42.673793Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.654146Z"
    },
    "papermill": {
     "duration": 0.030216,
     "end_time": "2023-10-04T21:59:39.796908",
     "exception": false,
     "start_time": "2023-10-04T21:59:39.766692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "group_kfold = GroupKFold(n_splits=CFG[\"kfolds\"])\n",
    "groups = df_file_list[\"series_id\"]\n",
    "train_folds=[]\n",
    "val_folds=[]\n",
    "for i, (train_index, test_index) in enumerate(group_kfold.split(df_file_list, groups=groups)):\n",
    "    train_folds.append(train_index)\n",
    "    val_folds.append(test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2489c5-272b-496a-a050-ee26d355d879",
   "metadata": {},
   "source": [
    "# Define Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbfa0be-c35c-4ebe-a714-924c69aa287d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.675343Z",
     "iopub.status.busy": "2023-11-14T22:36:42.674956Z",
     "iopub.status.idle": "2023-11-14T22:36:42.683950Z",
     "shell.execute_reply": "2023-11-14T22:36:42.683568Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.675325Z"
    }
   },
   "outputs": [],
   "source": [
    "###windowed dataset for training\n",
    "def map_series_to_x(series_id):\n",
    "    arr = np.load(f\"{TRAINING_DIR}/{series_id.decode('UTF-8')}.npy\")[:,1:-1]\n",
    "    s_name = series_id.decode('UTF-8')\n",
    "    if s_name in CFG[\"restricted_series\"].keys():\n",
    "        arr = arr[CFG[\"restricted_series\"][s_name][0]:CFG[\"restricted_series\"][s_name][1],:]\n",
    "        remainder = arr.shape[0]%12\n",
    "        if remainder != 0: arr = arr[:-remainder,:]\n",
    "    return arr\n",
    "\n",
    "def get_windowed_x(arr, samples_per_day=CFG[\"samples_per_day\"]):\n",
    "    tensor_ds = (tf.data.Dataset.from_tensor_slices(arr)\n",
    "                             .window(size=CFG[\"block_size\"],\n",
    "                                     shift=(CFG[\"block_size\"]//samples_per_day),\n",
    "                                     drop_remainder=True)\n",
    "                             .flat_map(lambda x:x.batch(CFG[\"block_size\"]),)\n",
    "    )\n",
    "    return tensor_ds\n",
    "\n",
    "def map_series_to_y(series_id):\n",
    "    arr = np.load(f\"{TRAINING_DIR}/{series_id.decode('UTF-8')}.npy\")[:,-1]\n",
    "    s_name = series_id.decode('UTF-8')\n",
    "    if s_name in CFG[\"restricted_series\"].keys():\n",
    "        arr = arr[CFG[\"restricted_series\"][s_name][0]:CFG[\"restricted_series\"][s_name][1]]\n",
    "        remainder = arr.shape[0]%12\n",
    "        if remainder != 0: arr = arr[:-remainder]\n",
    "        \n",
    "    arr = arr.reshape(-1, CFG[\"label_agg_size\"])\n",
    "    arr = np.where(arr==2,-1,arr)\n",
    "    arr = arr.mean(axis=-1)\n",
    "    #set class label for patches based on the mean value of its steps\n",
    "    arr = np.where(arr>=CFG[\"label_threshold\"],1,arr)\n",
    "    arr = np.where( (-CFG[\"label_threshold\"]<=arr) & (arr<=CFG[\"label_threshold\"]),0,arr)\n",
    "    arr = np.where(arr<=-CFG[\"label_threshold\"],2,arr)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def get_windowed_y(arr, samples_per_day=CFG['samples_per_day']):\n",
    "    tensor_ds = (tf.data.Dataset.from_tensor_slices(arr)\n",
    "                             .window(size=CFG[\"block_size\"]//CFG[\"label_agg_size\"],\n",
    "                                     shift=(CFG[\"block_size\"]//CFG[\"label_agg_size\"])//samples_per_day,\n",
    "                                     drop_remainder=True)\n",
    "                             .flat_map(lambda x:x.batch(CFG[\"block_size\"]//CFG[\"label_agg_size\"]))\n",
    "    )\n",
    "    return tensor_ds\n",
    "    \n",
    "\n",
    "def get_windowed_dataset(series_ids, shuffle=False, samples_per_day=CFG['samples_per_day']):\n",
    "    # file_paths: series containing filenames of saved npy files\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series_ids)\n",
    "    dataset = dataset.interleave(lambda item: tf.data.Dataset.zip(\n",
    "                                            (get_windowed_x(tf.numpy_function(map_series_to_x, [item], tf.float32),samples_per_day),\n",
    "                                             get_windowed_y(tf.numpy_function(map_series_to_y, [item], tf.float32),samples_per_day)\n",
    "                                            )),\n",
    "                                  num_parallel_calls=tf.data.AUTOTUNE\n",
    "                                 ).cache()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(GPU_BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d5e147a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.685008Z",
     "iopub.status.busy": "2023-11-14T22:36:42.684630Z",
     "iopub.status.idle": "2023-11-14T22:36:42.690120Z",
     "shell.execute_reply": "2023-11-14T22:36:42.689687Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.684991Z"
    },
    "papermill": {
     "duration": 0.016001,
     "end_time": "2023-10-04T21:59:39.864754",
     "exception": false,
     "start_time": "2023-10-04T21:59:39.848753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset for calculating mAP during CV.\n",
    "def map_path_to_x(fname):\n",
    "    arr = np.load(f\"{DATA_DIR}/{fname.decode('UTF-8')}\")[:,1:-1]\n",
    "    arr = np.nan_to_num(arr)\n",
    "    return arr\n",
    "\n",
    "def map_path_to_y(fname):\n",
    "    arr = np.load(f\"{DATA_DIR}/{fname.decode('UTF-8')}\")[:,-1]\n",
    "    arr = arr.reshape(CFG[\"block_size\"]//CFG[\"label_agg_size\"], CFG[\"label_agg_size\"])\n",
    "    arr = np.where(arr==2,-1,arr)\n",
    "    arr = arr.mean(axis=-1)\n",
    "    #set class label for patches based on the mean value of its steps\n",
    "    arr = np.where(arr>=CFG[\"label_threshold\"],1,arr)\n",
    "    arr = np.where( (-CFG[\"label_threshold\"]<=arr) & (arr<=CFG[\"label_threshold\"]),0,arr)\n",
    "    arr = np.where(arr<=-CFG[\"label_threshold\"],2,arr)\n",
    "    return arr\n",
    "\n",
    "#NOTE:  val_data must be deterministic and not shuffled, else `post_proc` and `calc_metric` silently fail\n",
    "def get_dataset(file_paths, shuffle=False, deterministic=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "    dataset = dataset.map(lambda item: (tf.ensure_shape(tf.numpy_function(map_path_to_x, [item], tf.float32), \n",
    "                                                        (CFG[\"block_size\"], CFG[\"n_feats\"])),\n",
    "                                                        tf.numpy_function(map_path_to_y, [item], tf.float32)),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "                          deterministic=deterministic).cache()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(GPU_BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "202b8919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.692920Z",
     "iopub.status.busy": "2023-11-14T22:36:42.692400Z",
     "iopub.status.idle": "2023-11-14T22:36:42.696705Z",
     "shell.execute_reply": "2023-11-14T22:36:42.696332Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.692898Z"
    },
    "papermill": {
     "duration": 0.013933,
     "end_time": "2023-10-04T21:59:39.884239",
     "exception": false,
     "start_time": "2023-10-04T21:59:39.870306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_path_to_step_idx(fname):\n",
    "    arr = np.load(f\"{DATA_DIR}/{fname.decode('UTF-8')}\")[:,0]\n",
    "    arr = arr.reshape(CFG[\"block_size\"]//CFG[\"label_agg_size\"], CFG[\"label_agg_size\"])\n",
    "    arr = np.floor(arr.mean(axis=-1)).astype(np.int32)\n",
    "    return arr\n",
    "\n",
    "def get_step_idxs(file_paths):\n",
    "    # file_paths: series containing filenames of saved npy files\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "    dataset = dataset.map(lambda item:  tf.numpy_function(map_path_to_step_idx, [item], tf.int32),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE,\n",
    "                          deterministic=True)\n",
    "    dataset = dataset.batch(GPU_BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e56fa",
   "metadata": {
    "papermill": {
     "duration": 0.00548,
     "end_time": "2023-10-04T21:59:39.895513",
     "exception": false,
     "start_time": "2023-10-04T21:59:39.890033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6170b-0695-4204-9e5c-dd62263ec9fa",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8746f48-4e1a-42a4-a4d4-9cb5c8ff1fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.697710Z",
     "iopub.status.busy": "2023-11-14T22:36:42.697202Z",
     "iopub.status.idle": "2023-11-14T22:36:42.701583Z",
     "shell.execute_reply": "2023-11-14T22:36:42.701196Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.697694Z"
    }
   },
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class AugRoll(layers.Layer):\n",
    "    def __init__(self, roll_range):\n",
    "        super().__init__()\n",
    "        self.r=roll_range\n",
    "        \n",
    "    def call(self,x, training=False):\n",
    "        if training:\n",
    "            return tf.roll(x, shift=tf.random.uniform([],-self.r,self.r+1,tf.int32), axis=-1)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # Update the config with the custom layer's parameters\n",
    "        config.update({ \"roll_range\": self.r})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1843e393-cb74-43e1-808f-71d9ac758d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.708167Z",
     "iopub.status.busy": "2023-11-14T22:36:42.707580Z",
     "iopub.status.idle": "2023-11-14T22:36:42.713273Z",
     "shell.execute_reply": "2023-11-14T22:36:42.712564Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.708151Z"
    }
   },
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, patch_size, stride, d_out, layer_norm=True):\n",
    "        super().__init__()\n",
    "        self.k = patch_size; self.s = stride; self.d = d_out; self.l = layer_norm\n",
    "        \n",
    "        self.proj = layers.Dense(d_out)\n",
    "        self.layer_norm = layers.LayerNormalization() if layer_norm else layers.Identity()\n",
    "    \n",
    "    def call(self,x, training=None): # x shape: (B,L,C)\n",
    "        x = tf.expand_dims(x,axis=1) # (B,1,L,C)\n",
    "        x = tf.image.extract_patches(x, [1,1,self.k,1], [1,1,self.s,1], [1,1,1,1], padding=\"SAME\") # (B,1,L/s,C*k)\n",
    "        x = tf.squeeze(x, axis=1) # (B,L/s,C*k)\n",
    "        x = self.proj(x) # (B,L/s,d_out)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # Update the config with the custom layer's parameters\n",
    "        config.update({ \"patch_size\": self.k,\n",
    "                        \"stride\": self.s,\n",
    "                        \"d_out\": self.d,\n",
    "                        # \"d_out\": self.proj,\n",
    "                        \"layer_norm\": self.l}\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e335b713-e48d-4f4a-9876-da9b7ef34be6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.714268Z",
     "iopub.status.busy": "2023-11-14T22:36:42.713797Z",
     "iopub.status.idle": "2023-11-14T22:36:42.719514Z",
     "shell.execute_reply": "2023-11-14T22:36:42.718941Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.714253Z"
    }
   },
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class MixFFN(layers.Layer):\n",
    "    def __init__(self,d_in, d_hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in; self.d_hidden = d_hidden; self.dropout = dropout\n",
    "        \n",
    "        self.dense1 = layers.Dense(d_hidden)\n",
    "        self.dwconv = layers.DepthwiseConv1D(3, depth_multiplier=1, padding='same', activation=\"gelu\")\n",
    "        self.dense2 = layers.Dense(d_in)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.add = layers.Add()\n",
    "        \n",
    "    def call(self, x_in, training=None):\n",
    "        x = self.dense1(x_in)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.add([x_in,x])\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # Update the config with the custom layer's parameters\n",
    "        config.update({ \"d_in\": self.d_in,\n",
    "                        \"d_hidden\": self.d_hidden,\n",
    "                        \"dropout\": self.dropout}\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c635f6d7-4ded-4296-9343-af4e94c06dd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.720347Z",
     "iopub.status.busy": "2023-11-14T22:36:42.720187Z",
     "iopub.status.idle": "2023-11-14T22:36:42.725920Z",
     "shell.execute_reply": "2023-11-14T22:36:42.725320Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.720333Z"
    }
   },
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class EfficientMHSA(layers.Layer):\n",
    "    def __init__(self, input_dim, n_heads, r_ratio, mha_dropout=CFG[\"mha_dropout\"]):\n",
    "        super().__init__()\n",
    "        self.input_dim=input_dim; self.n_heads = n_heads; self.r_ratio = r_ratio; self.mha_d = mha_dropout\n",
    "        \n",
    "        self.reshape = layers.Reshape((-1,input_dim*r_ratio)) if r_ratio!=1 else layers.Identity()\n",
    "        self.key_proj = layers.Dense(input_dim) if r_ratio!=1 else layers.Identity()\n",
    "        self.value_proj = layers.Dense(input_dim) if r_ratio!=1 else layers.Identity()\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.att = layers.MultiHeadAttention(n_heads, input_dim, input_dim, dropout=mha_dropout)\n",
    "        \n",
    "    def call(self,x, training=None):\n",
    "        k = self.norm( self.key_proj( self.reshape(x)))\n",
    "        v = self.norm( self.value_proj( self.reshape(x)))\n",
    "        output = self.att(x,v,k)\n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # Update the config with the custom layer's parameters\n",
    "        config.update({ \"input_dim\": self.input_dim,\n",
    "                        \"n_heads\": self.n_heads,\n",
    "                        \"r_ratio\": self.r_ratio,\n",
    "                        \"mha_dropout\": self.mha_d}\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72baaa96-a047-4480-94ec-d5d03254fd6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.726780Z",
     "iopub.status.busy": "2023-11-14T22:36:42.726597Z",
     "iopub.status.idle": "2023-11-14T22:36:42.732733Z",
     "shell.execute_reply": "2023-11-14T22:36:42.732203Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.726766Z"
    }
   },
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class SegformerEncoderBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self, channels, n_layers, n_heads, r_ratio, exp_ratio):\n",
    "        '''\n",
    "        channels: number of input (and output) channels\n",
    "        n_layers: number of encoder layers for this block\n",
    "        n_heads: number of self-attention heads for each encoder layer\n",
    "        r_ratio: reduction ratio for efficient self-attention\n",
    "        exp_ratio: expansion ratio of mix-FFN layer\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.channels=channels\n",
    "        self.n_heads = n_heads\n",
    "        self.r_ratio=r_ratio,\n",
    "        self.exp_ratio = exp_ratio\n",
    "        self.mix_ffn = MixFFN(d_in=channels, d_hidden=channels*exp_ratio, dropout=CFG[\"encoder_dropout\"])\n",
    "        self.esa = EfficientMHSA(channels, n_heads, r_ratio)\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "        self.add = layers.Add()\n",
    "        self.n_layers=n_layers\n",
    "        \n",
    "    def call(self,x, training=None):\n",
    "        for _ in range(self.n_layers):\n",
    "            x_skip = x\n",
    "            x = self.layer_norm(x)\n",
    "            x = self.esa(x)\n",
    "            x = self.add([x_skip,x])\n",
    "            x_skip = x\n",
    "            x = self.layer_norm(x)\n",
    "            x = self.mix_ffn(x)\n",
    "            x = self.add([x_skip,x])\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # Update the config with the custom layer's parameters\n",
    "        config.update({ \"channels\": self.channels,\n",
    "                        \"n_heads\": self.n_heads,\n",
    "                        \"r_ratio\": self.r_ratio,\n",
    "                        \"n_layers\": self.n_layers,\n",
    "                        \"exp_ratio\": self.exp_ratio}\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e986fa-5656-4808-9ba5-6c84d0c310d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.733556Z",
     "iopub.status.busy": "2023-11-14T22:36:42.733400Z",
     "iopub.status.idle": "2023-11-14T22:36:42.738431Z",
     "shell.execute_reply": "2023-11-14T22:36:42.737769Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.733542Z"
    }
   },
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class UnifyFeatures(layers.Layer):\n",
    "    def __init__(self, decoder_channels, stride):\n",
    "        super().__init__()\n",
    "        self.d=decoder_channels\n",
    "        self.s=stride\n",
    "        self.linear = layers.Dense(decoder_channels) \n",
    "        self.dropout = layers.Dropout(CFG[\"decoder_dropout\"])\n",
    "        self.upsample = layers.UpSampling1D(stride) if stride > 1 else layers.Identity()\n",
    "        \n",
    "    def call(self,x, training=None):\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # Update the config with the custom layer's parameters\n",
    "        config.update({ \"decoder_channels\": self.d,\n",
    "                        \"stride\": self.s}\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca1ce6fb-9b26-4827-96cd-45fd036bed30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.739289Z",
     "iopub.status.busy": "2023-11-14T22:36:42.739088Z",
     "iopub.status.idle": "2023-11-14T22:36:42.749719Z",
     "shell.execute_reply": "2023-11-14T22:36:42.749066Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.739274Z"
    }
   },
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class SegFormer(tf.keras.Model):\n",
    "    def __init__(self, patches, strides, channels, enc_layers, heads, r_ratios, exp_ratios, d_decoder, aug_roll):\n",
    "        '''\n",
    "        patches (list): list of patch sizes for each of the embedding layers\n",
    "        strides (list): list of strides for each embedding layer (and their corresponding upsampling layer)\n",
    "        channels (list): list of channel dimensions for each encoder block\n",
    "        enc_layers (list): list of layers in each encoder block\n",
    "        heads (list): list of attention head numbers for each encoder block\n",
    "        r_ratios (list): list of EfficientMHSA reduction ratios in each encoder block \n",
    "        exp_ratios (list): list of MixFFN expansion ratios in each encoder block\n",
    "        d_decoder (int): final channel size for the decoder\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.p=patches\n",
    "        self.s=strides\n",
    "        self.c=channels\n",
    "        self.enc=enc_layers\n",
    "        self.h=heads\n",
    "        self.r=r_ratios\n",
    "        self.exp=exp_ratios\n",
    "        self.dec=d_decoder\n",
    "        self.roll=aug_roll\n",
    "        ### hyperparameter lists must have the same length\n",
    "        assert len( set(\n",
    "            [len(patches), len(strides), len(channels), len(enc_layers), len(heads), len(r_ratios), len(exp_ratios)]\n",
    "        )) == 1,\"SegFormer hyperparameter lists must be the same size!\"\n",
    "        \n",
    "        self.aug_roll = AugRoll(aug_roll)\n",
    "        self.patch_embed = [PatchEmbedding(p,s,c) for p,s,c in zip(patches, strides, channels)]  \n",
    "        self.enc_block = [SegformerEncoderBlock(c,l,h,r,e) for c,l,h,r,e in zip(channels,enc_layers,heads,r_ratios,exp_ratios)]\n",
    "        ####### TODO: make this generalizable to any list size\n",
    "        self.unify = [UnifyFeatures(d_decoder,1), UnifyFeatures(d_decoder,strides[1]), \n",
    "                      UnifyFeatures(d_decoder,strides[1]*strides[2]),UnifyFeatures(d_decoder,strides[1]*strides[2]*strides[3])] \n",
    "        self.mlp_fuse = layers.Dense(d_decoder)\n",
    "        self.fuse_do = layers.Dropout(CFG[\"fuse_dropout\"])\n",
    "        self.classifier = layers.Dense(3) #shape: (B, L/strides[0], 3)\n",
    "        self.concat = layers.Concatenate()\n",
    "        self.bn= layers.BatchNormalization()\n",
    "        self.act = layers.Activation(\"relu\")\n",
    "          \n",
    "    \n",
    "        ###### WORKAROUND TO \"FIX\" MODEL.SUMMARY() ######\n",
    "        self.input_layer = tf.keras.layers.Input(shape=(CFG['block_size'],CFG['n_feats']), name=\"input_layer\")\n",
    "        input_shape = self.input_layer\n",
    "        outputs_shape = self.call(input_shape)\n",
    "        super().__init__(inputs=input_shape,outputs=outputs_shape)\n",
    "        #################################################\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        x = self.aug_roll(x,training)\n",
    "        x = self.patch_embed[0](x)\n",
    "        out_0 = self.enc_block[0](x)\n",
    "        x = self.patch_embed[1](out_0)\n",
    "        out_1 = self.enc_block[1](x)\n",
    "        x = self.patch_embed[2](out_1)\n",
    "        out_2 = self.enc_block[2](x)\n",
    "        x = self.patch_embed[3](out_2)\n",
    "        out_3 = self.enc_block[3](x)\n",
    "        x = self.concat([self.unify[0](out_0), self.unify[1](out_1), self.unify[2](out_2), self.unify[3](out_3)]) \n",
    "        x = self.mlp_fuse(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fuse_do(x)\n",
    "        x = self.act(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        # Update the config with the custom layer's parameters\n",
    "        config.update({ \"patches\": self.p,\n",
    "                        \"strides\": self.s,\n",
    "                        \"channels\": self.c,\n",
    "                        \"enc_layers\": self.enc,\n",
    "                        \"heads\": self.h,\n",
    "                        \"r_ratios\": self.r,\n",
    "                        \"exp_ratios\": self.exp,\n",
    "                        \"d_decoder\": self.dec,\n",
    "                        \"aug_roll\": self.roll}\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ab54c53-dbff-439a-a76f-9c1043e9114d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:42.750763Z",
     "iopub.status.busy": "2023-11-14T22:36:42.750592Z",
     "iopub.status.idle": "2023-11-14T22:36:46.097856Z",
     "shell.execute_reply": "2023-11-14T22:36:46.097229Z",
     "shell.execute_reply.started": "2023-11-14T22:36:42.750749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seg_former_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_layer (InputLayer)       [(None, 17280, 5)]   0           []                               \n",
      "                                                                                                  \n",
      " aug_roll (AugRoll)             (None, 17280, 5)     0           ['input_layer[0][0]']            \n",
      "                                                                                                  \n",
      " patch_embedding (PatchEmbeddin  (None, 2880, 12)    636         ['aug_roll[0][0]']               \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " segformer_encoder_block (Segfo  (None, 2880, 12)    4608        ['patch_embedding[0][0]']        \n",
      " rmerEncoderBlock)                                                                                \n",
      "                                                                                                  \n",
      " patch_embedding_1 (PatchEmbedd  (None, 720, 24)     2376        ['segformer_encoder_block[0][0]']\n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " segformer_encoder_block_1 (Seg  (None, 720, 24)     17280       ['patch_embedding_1[0][0]']      \n",
      " formerEncoderBlock)                                                                              \n",
      "                                                                                                  \n",
      " patch_embedding_2 (PatchEmbedd  (None, 180, 48)     9360        ['segformer_encoder_block_1[0][0]\n",
      " ing)                                                            ']                               \n",
      "                                                                                                  \n",
      " segformer_encoder_block_2 (Seg  (None, 180, 48)     66816       ['patch_embedding_2[0][0]']      \n",
      " formerEncoderBlock)                                                                              \n",
      "                                                                                                  \n",
      " patch_embedding_3 (PatchEmbedd  (None, 60, 96)      37152       ['segformer_encoder_block_2[0][0]\n",
      " ing)                                                            ']                               \n",
      "                                                                                                  \n",
      " segformer_encoder_block_3 (Seg  (None, 60, 96)      262656      ['patch_embedding_3[0][0]']      \n",
      " formerEncoderBlock)                                                                              \n",
      "                                                                                                  \n",
      " unify_features (UnifyFeatures)  (None, 2880, 64)    832         ['segformer_encoder_block[0][0]']\n",
      "                                                                                                  \n",
      " unify_features_1 (UnifyFeature  (None, 2880, 64)    1600        ['segformer_encoder_block_1[0][0]\n",
      " s)                                                              ']                               \n",
      "                                                                                                  \n",
      " unify_features_2 (UnifyFeature  (None, 2880, 64)    3136        ['segformer_encoder_block_2[0][0]\n",
      " s)                                                              ']                               \n",
      "                                                                                                  \n",
      " unify_features_3 (UnifyFeature  (None, 2880, 64)    6208        ['segformer_encoder_block_3[0][0]\n",
      " s)                                                              ']                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2880, 256)    0           ['unify_features[0][0]',         \n",
      "                                                                  'unify_features_1[0][0]',       \n",
      "                                                                  'unify_features_2[0][0]',       \n",
      "                                                                  'unify_features_3[0][0]']       \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 2880, 64)     16448       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 2880, 64)    256         ['dense_16[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 2880, 64)     0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 2880, 64)     0           ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 2880, 3)      195         ['activation[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 429,559\n",
      "Trainable params: 429,431\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "segmodel = SegFormer(patches=CFG[\"patches\"], \n",
    "                     strides=CFG[\"strides\"], \n",
    "                     channels=CFG[\"channels\"], \n",
    "                     enc_layers=CFG[\"enc_layers\"], \n",
    "                     heads=CFG[\"heads\"], \n",
    "                     r_ratios=CFG[\"r_ratios\"], \n",
    "                     exp_ratios=CFG[\"exp_ratios\"],\n",
    "                     d_decoder=CFG[\"d_decoder\"],\n",
    "                     aug_roll=CFG[\"roll\"])\n",
    "segmodel.build(input_shape=(None,\n",
    "                            CFG['block_size'],\n",
    "                            CFG['n_feats']))\n",
    "\n",
    "### fixed keras summary with: https://github.com/keras-team/keras/issues/13782#issuecomment-1446475317\n",
    "segmodel.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b253433-5143-4210-9e46-932b26c99dbf",
   "metadata": {},
   "source": [
    "# Define custom losses, postprocessing, and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14686e2",
   "metadata": {
    "papermill": {
     "duration": 0.007867,
     "end_time": "2023-10-04T21:59:45.699888",
     "exception": false,
     "start_time": "2023-10-04T21:59:45.692021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions to generate valid submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f2c063a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:46.098859Z",
     "iopub.status.busy": "2023-11-14T22:36:46.098679Z",
     "iopub.status.idle": "2023-11-14T22:36:46.106289Z",
     "shell.execute_reply": "2023-11-14T22:36:46.105461Z",
     "shell.execute_reply.started": "2023-11-14T22:36:46.098842Z"
    },
    "papermill": {
     "duration": 0.015932,
     "end_time": "2023-10-04T21:59:45.767075",
     "exception": false,
     "start_time": "2023-10-04T21:59:45.751143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preproc(df):\n",
    "    \"\"\" \n",
    "    input: pd.DataFrame -- columns = (\"series_id, step, anglez, enmo, lids\") -- 17280 steps\n",
    "    output: tuple of np.arrays (x, step_idxs) -- shapes == (n_patches, -1)\n",
    "    \"\"\"\n",
    "    step_idx = df[\"step\"].to_numpy(dtype=np.int32)\n",
    "    if step_idx.shape[0]<17280:\n",
    "        pad = np.zeros((17280-step_idx.shape[0],),dtype=np.float32)\n",
    "        step_idx = np.concatenate((pad,step_idx),axis=None)\n",
    "    if CFG['n_feats']==3:\n",
    "        x = df[[\"anglez\", \"enmo\", \"lids\"]].to_numpy(dtype=np.float32)\n",
    "    elif CFG['n_feats']==5:\n",
    "        x = df[[\"anglez\", \"enmo\", \"lids\", \"t_sin\", \"t_cos\"]].to_numpy(dtype=np.float32)\n",
    "    if x.shape[0]<17280:\n",
    "        pad = np.zeros((17280-x.shape[0],x.shape[1]),dtype=np.float32)\n",
    "        x = np.vstack((pad,x))\n",
    "    x = np.nan_to_num(x) # replace NaNs (from rolling functions) with zeroes \n",
    "    x = np.expand_dims(x, axis=0) # add batch dimension expected by the model\n",
    "\n",
    "    step_idx = step_idx.reshape(-1, LABEL_DOWNSAMPLE)\n",
    "    step_idx = np.floor(step_idx.mean(axis=-1)).astype(np.int32)\n",
    "    \n",
    "    return x, step_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab0188c9-5917-4342-ab44-842422582fbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:46.107169Z",
     "iopub.status.busy": "2023-11-14T22:36:46.107003Z",
     "iopub.status.idle": "2023-11-14T22:36:46.119943Z",
     "shell.execute_reply": "2023-11-14T22:36:46.119232Z",
     "shell.execute_reply.started": "2023-11-14T22:36:46.107154Z"
    }
   },
   "outputs": [],
   "source": [
    "# handle 1 column at a time.\n",
    "##### i.e., pass `onset` and `wakeup` separately (from:<<  onset, wakeup = np.hsplit(a[:,:],2)   >> )\n",
    "def parse_logits(col, min_distance=30, min_logit=0):\n",
    "    # get sorted array of top logit idxs\n",
    "    col_sorted = np.argsort(col, axis = None)[::-1]\n",
    "    hit_idx = np.flatnonzero(col > min_logit)\n",
    "    if hit_idx.shape[0]==0:\n",
    "        return None, None\n",
    "    \n",
    "    top_hit_idx = col_sorted[np.isin(col_sorted, hit_idx)]\n",
    "    \n",
    "    # filter logit idxs to prevent submitting neighboring predictions\n",
    "    distance_matrix = np.abs(top_hit_idx.reshape(-1,1) - top_hit_idx.reshape(-1,1).T)\n",
    "    bool_matrix = np.where(distance_matrix>=min_distance, False, True)\n",
    "    # prediction idxs\n",
    "    pred_idxs = top_hit_idx[(np.argmax(bool_matrix,axis=1) - np.arange(bool_matrix.shape[0]))==0]\n",
    "    # prediction confidences\n",
    "    pred_logits = col[pred_idxs].flatten() \n",
    "    \n",
    "    return pred_idxs, pred_logits\n",
    "\n",
    "def postproc(x, step_idx, series_id):\n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        x: np.array with shape (n_patches,2).  column 0 = onset logits, column 1 = wakeup logits\n",
    "        step_idx: np.array for mapping argmax to step\n",
    "        series_id: string\n",
    "    output: None OR pd.Dataframe -- columns = (series_id, step, event, score)\n",
    "    \"\"\"\n",
    "    onset, wakeup = np.hsplit(x,2)\n",
    "\n",
    "    onset_idxs, onset_logits = parse_logits(onset, min_distance=CFG[\"min_pred_distance\"], min_logit=CFG[\"logit_threshold\"])\n",
    "    wakeup_idxs, wakeup_logits = parse_logits(wakeup, min_distance=CFG[\"min_pred_distance\"], min_logit=CFG[\"logit_threshold\"])\n",
    "    \n",
    "    # exit if no wakeup or onset predictions\n",
    "    if (onset_idxs is None) and (wakeup_idxs is None):\n",
    "        return None\n",
    "    \n",
    "    # verify step_idx is correct shape for extending step_idx_list\n",
    "    if len(step_idx.shape) == 2: \n",
    "        step_idx = step_idx.flatten()\n",
    "        \n",
    "        \n",
    "    # initialize lists to hold values until dataframe creation\n",
    "    series_id_list, step_idx_list, event_list, score_list = [], [], [], []\n",
    "    \n",
    "    if onset_idxs is not None:\n",
    "        \n",
    "        step_idx_list.extend(step_idx[onset_idxs])\n",
    "        score_list.extend(onset_logits)\n",
    "        series_id_list.extend([series_id] * onset_idxs.shape[0])\n",
    "        event_list.extend([\"onset\"] * onset_idxs.shape[0])\n",
    "    \n",
    "    if wakeup_idxs is not None:\n",
    "        \n",
    "        step_idx_list.extend(step_idx[wakeup_idxs])\n",
    "        score_list.extend(wakeup_logits)\n",
    "        series_id_list.extend([series_id] * wakeup_idxs.shape[0])\n",
    "        event_list.extend([\"wakeup\"] * wakeup_idxs.shape[0])\n",
    "        \n",
    "    df_pred = pd.DataFrame({\n",
    "        \"series_id\" : series_id_list,\n",
    "        \"step\" : step_idx_list,\n",
    "        \"event\" : event_list,\n",
    "        \"score\" : score_list   \n",
    "    })\n",
    "    \n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78f426",
   "metadata": {
    "papermill": {
     "duration": 0.008147,
     "end_time": "2023-10-04T21:59:45.812682",
     "exception": false,
     "start_time": "2023-10-04T21:59:45.804535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compile and train the model\n",
    "\n",
    "### TODO: CONFIGURE CALLBACKS AND MODEL SAVING AND HISTORY for training loop\n",
    "\n",
    "- consider learning rate scheduler callback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07f7c57a-5363-400b-aeed-b59d221f4020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:46.121735Z",
     "iopub.status.busy": "2023-11-14T22:36:46.121144Z",
     "iopub.status.idle": "2023-11-14T22:36:46.139055Z",
     "shell.execute_reply": "2023-11-14T22:36:46.138399Z",
     "shell.execute_reply.started": "2023-11-14T22:36:46.121716Z"
    }
   },
   "outputs": [],
   "source": [
    "TOLERANCES = {\"onset\":  [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],\n",
    "              \"wakeup\": [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]}\n",
    "\n",
    "def calc_metric(val_idxs):\n",
    "    # only keep indices for window 19\n",
    "    val_idxs = list( set(val_idxs) - set(df_file_list.loc[df_file_list.window != 19].index))\n",
    "\n",
    "    ### get \"solution\" dataframe for computing the metric. \n",
    "    val_series_ids = (df_file_list.iloc[val_idxs].series_id.unique().tolist())\n",
    "    df_pred_list = []\n",
    "    for series_id in val_series_ids:\n",
    "        series_splits = (pl.scan_parquet(\"/notebooks/input/train_series.parquet\")\n",
    "                        # polars LazyFrame\n",
    "                        .select([\"series_id\",\"step\",\"timestamp\"])\n",
    "                        .filter(pl.col(\"series_id\")==series_id) \n",
    "                        .with_columns((pl.col(\"timestamp\")\n",
    "                                      .str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%S%z\")), #convert timezone to UTC\n",
    "                        )\n",
    "                        .filter(pl.col(\"timestamp\").dt.time() == datetime.time(19,0,0))\n",
    "                        .drop(\"timestamp\")\n",
    "                        .collect()\n",
    "                        .to_pandas()\n",
    "                         # pandas DataFrame\n",
    "                        .assign(start_idx = lambda x: np.where(x[\"step\"].astype(np.int32)-17280 > 0,\n",
    "                                                           x[\"step\"].astype(np.int32)-17280,\n",
    "                                                           0)\n",
    "                        )\n",
    "                        .rename(columns={\"step\":\"end_idx\"})\n",
    "                        .loc[:,[\"series_id\",\"start_idx\",\"end_idx\"]]\n",
    "        )\n",
    "\n",
    "        df_series=(pl.scan_parquet(\"/notebooks/input/train_series.parquet\")\n",
    "                        .filter(pl.col(\"series_id\")==series_id)\n",
    "                        .with_columns([\n",
    "                            (pl.col(\"timestamp\").str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%S%z\")), #parse timezone\n",
    "                            (pl.col(\"anglez\")\n",
    "                                 .diff(n=1,null_behavior=\"ignore\")\n",
    "                                 .fill_null(value=0)\n",
    "                                 .abs()\n",
    "                                 .alias(\"anglez\")),\n",
    "                            (pl.col(\"enmo\").clip(0.0,3.0).alias(\"lids\")),\n",
    "                            (pl.col(\"enmo\").clip(0.0,3.0).alias(\"enmo\")),\n",
    "                        ])\n",
    "                        # center anglez and enmo\n",
    "                        .with_columns([\n",
    "                            ((pl.col(\"anglez\")-pl.col(\"anglez\").mean())\n",
    "                                .alias(\"anglez\")),\n",
    "                            ((pl.col(\"enmo\")-pl.col(\"enmo\").mean())\n",
    "                                .alias(\"enmo\")),\n",
    "                            (((pl.col(\"timestamp\").dt.hour()*pl.lit(60)+pl.col(\"timestamp\").dt.minute())*pl.lit(2*np.pi/1440))\n",
    "                                 .sin().alias(\"t_sin\")\n",
    "                            ),\n",
    "                            (((pl.col(\"timestamp\").dt.hour()*pl.lit(60)+pl.col(\"timestamp\").dt.minute())*pl.lit(2*np.pi/1440))\n",
    "                                 .cos().alias(\"t_cos\")\n",
    "                        )\n",
    "                        ])\n",
    "                        .with_columns((pl.col(\"anglez\")/pl.col(\"anglez\").std()).alias(\"anglez\"))\n",
    "                        .drop(\"timestamp\")\n",
    "                        .collect()\n",
    "                        .to_pandas()\n",
    "                        .assign(lids= lambda x: (100 / (np.maximum(0., x['lids'] - 0.02).rolling(120, center=True, min_periods=1).agg('sum') + 1).rolling(360, center=True, min_periods=1).agg('mean').astype(np.float32)),)\n",
    "                        .assign(lids =lambda x: (x.lids/50)-1)\n",
    "                        .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        for i in series_splits.index:\n",
    "            #pad split_0 if it's reasonably long.  otherwise discard it.\n",
    "            if i == 0: continue\n",
    "\n",
    "            day=(df_series[df_series[\"step\"].between(series_splits.iloc[i,-2],\n",
    "                                                     series_splits.iloc[i,-1], inclusive=\"left\")]\n",
    "                         .loc[:,[\"series_id\", \"step\", \"anglez\", \"enmo\", \"lids\", \"t_sin\", \"t_cos\"]]\n",
    "            )\n",
    "\n",
    "\n",
    "        ### Inference\n",
    "            x, step_idx  = preproc(day)\n",
    "            logits = tf.squeeze(segmodel.predict(x,verbose=0)).numpy()[:,1:]\n",
    "            df_pred = postproc(logits, step_idx, series_id)\n",
    "            # postproc may return <None> if logits don't meet criteria \n",
    "            if df_pred is not None:\n",
    "                df_pred_list.append(df_pred)\n",
    "    \n",
    "    \n",
    "    df_true = (pd.read_csv(EVENTS_PATH)\n",
    "                   .query(\"series_id.isin(@val_series_ids)\"))\n",
    "    \n",
    "    if len(df_pred_list) > 0:\n",
    "        df_val_preds = (pd.concat(df_pred_list, axis=0, ignore_index=True)\n",
    "                           .reset_index()\n",
    "                           .rename(columns={\"index\":\"row_id\"}))   \n",
    "    else: return 0.0\n",
    "        \n",
    "    return competition_score(  solution = df_true,\n",
    "                                submission = df_val_preds,\n",
    "                                tolerances= TOLERANCES,\n",
    "                                series_id_column_name = \"series_id\",\n",
    "                                time_column_name = \"step\",\n",
    "                                event_column_name = \"event\",\n",
    "                                score_column_name = \"score\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e25d9a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:46.139951Z",
     "iopub.status.busy": "2023-11-14T22:36:46.139786Z",
     "iopub.status.idle": "2023-11-14T22:36:46.145396Z",
     "shell.execute_reply": "2023-11-14T22:36:46.144850Z",
     "shell.execute_reply.started": "2023-11-14T22:36:46.139936Z"
    },
    "papermill": {
     "duration": 0.015298,
     "end_time": "2023-10-04T21:59:45.863897",
     "exception": false,
     "start_time": "2023-10-04T21:59:45.848599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: log the metric since I can tell it to write to the logger?\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        comp_metric = calc_metric(val_folds[k])\n",
    "        print(f\"********** Fold {k} epoch {epoch} AP = {comp_metric} **********\")\n",
    "        if LOG_WANDB:\n",
    "            wandb.log({f\"val_loss_k{k}\": logs[\"val_loss\"], \n",
    "                       f\"loss_k{k}\": logs[\"loss\"],\n",
    "                       f\"AP_k{k}\": comp_metric,\n",
    "                      })\n",
    "callbacks = [CustomCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e75b5127-0d44-456e-a286-abd9eb166fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:46.146583Z",
     "iopub.status.busy": "2023-11-14T22:36:46.146406Z",
     "iopub.status.idle": "2023-11-14T22:36:46.151208Z",
     "shell.execute_reply": "2023-11-14T22:36:46.150611Z",
     "shell.execute_reply.started": "2023-11-14T22:36:46.146568Z"
    }
   },
   "outputs": [],
   "source": [
    "# LEARNING RATE SCHEDULER\n",
    "def get_lr(batches=1500):\n",
    "    cos_lr = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        CFG[\"lr\"],\n",
    "        (CFG[\"epochs\"]*batches*GPU_BATCH_SIZE)//16,\n",
    "        alpha=CFG[\"lr\"]*1e-2,\n",
    "    )\n",
    "\n",
    "    lr = cos_lr if (CFG[\"lr_scheduler\"] == \"cos\") else CFG[\"lr\"]\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1385e2a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:46.152956Z",
     "iopub.status.busy": "2023-11-14T22:36:46.152504Z",
     "iopub.status.idle": "2023-11-14T22:36:46.433192Z",
     "shell.execute_reply": "2023-11-14T22:36:46.432523Z",
     "shell.execute_reply.started": "2023-11-14T22:36:46.152938Z"
    },
    "papermill": {
     "duration": 0.248548,
     "end_time": "2023-10-04T21:59:46.120309",
     "exception": false,
     "start_time": "2023-10-04T21:59:45.871761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "segmodel.compile(loss = loss, optimizer = tf.keras.optimizers.AdamW(learning_rate=get_lr()))\n",
    "# save initial weights so we can reset the model between CV folds\n",
    "init_weights = segmodel.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440be34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-14T22:36:46.434261Z",
     "iopub.status.busy": "2023-11-14T22:36:46.434066Z"
    },
    "papermill": {
     "duration": 7033.300677,
     "end_time": "2023-10-04T23:56:59.429355",
     "exception": false,
     "start_time": "2023-10-04T21:59:46.128678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    134/Unknown - 193s 685ms/step - loss: 0.1771"
     ]
    }
   ],
   "source": [
    "for k in range(CFG[\"kfolds\"]):\n",
    "    #get kth fold data\n",
    "    train_data = get_windowed_dataset(pd_series_ids.iloc[train_folds[k]].unique(), shuffle=4000)\n",
    "    val_data = get_windowed_dataset(pd_series_ids.iloc[val_folds[k]].unique(),samples_per_day=2)\n",
    "    # reset model for new fold\n",
    "    segmodel.compile(loss = loss,\n",
    "            optimizer = tf.keras.optimizers.AdamW(learning_rate=get_lr(\n",
    "                    (CFG[\"kfolds\"]-1)*110000//GPU_BATCH_SIZE//(CFG[\"kfolds\"]))) #an estimate\n",
    "    )\n",
    "    segmodel.set_weights(init_weights)\n",
    "    segmodel.fit(train_data, validation_data=val_data, epochs=CFG[\"epochs\"], callbacks=callbacks)\n",
    "    segmodel.save(f\"{OUTPUT_DIR}/segmodel_fold_{k}.tf\", overwrite=True,save_format=\"tf\")\n",
    "    \n",
    "    if CFG[\"ONE_FOLD\"]: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012674bd-26eb-4882-beff-982bad203799",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_WANDB: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc6c02",
   "metadata": {
    "papermill": {
     "duration": 0.325147,
     "end_time": "2023-10-04T23:57:00.754859",
     "exception": false,
     "start_time": "2023-10-04T23:57:00.429712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualize model output (logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e5341",
   "metadata": {
    "papermill": {
     "duration": 41.304374,
     "end_time": "2023-10-04T23:57:42.434087",
     "exception": false,
     "start_time": "2023-10-04T23:57:01.129713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = val_data.unbatch().take(40)\n",
    "for x, y in sample:\n",
    "    demo_preds = segmodel(tf.expand_dims(x,0), training=False)[0]\n",
    "    demo_labels = y\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax = fig.add_subplot()\n",
    "    ax.plot(np.arange(y.shape[0]), demo_labels*4, color='green', label=\"label\")\n",
    "    ax.plot(np.arange(y.shape[0]), demo_preds[:,1], color='b', linewidth=0.5, label=\"onset_preds\")\n",
    "    ax.plot(np.arange(y.shape[0]), demo_preds[:,2], color='xkcd:orange red', linewidth=0.5, label=\"wakeup_preds\")\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('step')\n",
    "    ax.set_ylabel('logits')\n",
    "    ax.set_facecolor('xkcd:off white')\n",
    "    labels = ax.get_xticklabels()\n",
    "    plt.setp(labels, rotation=40, fontsize=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9f703-b5fe-409b-b437-ecb7f08ccb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7099.611667,
   "end_time": "2023-10-04T23:57:47.527315",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-04T21:59:27.915648",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}